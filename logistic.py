# -*- coding: utf-8 -*-
"""RareEvent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMX5_JMq1ptOJIFBJGtNYvXSuQ53RQSK
"""
from augmentation import *
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
from sklearn.utils import shuffle
import pandas as pd
import numpy as np

AUGMENT = 'YES'
"""## Helper Methods"""

def scale_datasets(scaler, train_set, to_scale):
  scaler = scaler().fit(train_set)
  res = []
  for dataset in to_scale:
    res.append(scaler.transform(dataset))
  return res

def display_results(precision_recall_fscore_support):
  disp_df = pd.DataFrame(data=precision_recall_fscore_support, columns=['Pos Class'], index=['Precision', 'Recall', 'F-score', 'support'])
  print(disp_df)


"""### Load Data"""

data_path = 'data/processminer-rare-event-detection-data-augmentation.xlsx'
data_file = pd.ExcelFile(data_path)
data = pd.read_excel(data_file, 'data-(b)-4-min-ahead-conse-rmvd')

"""### Train-Test-Validation Split"""

SEED = 0
DATA_SPLIT_PCT = 0.20
label_name = 'y-4min-ahead'

data = data.drop(['time', 'x28', 'x61'], axis=1)
df_train, df_test = train_test_split(data, test_size=DATA_SPLIT_PCT, random_state=SEED)
df_train_x = df_train.drop([label_name], axis=1)
df_test_x = df_test.drop([label_name], axis=1)

train_y = df_train[label_name]
test_y = df_test[label_name]

#TODO make vars for labels
classifier = LogisticRegression(class_weight='balanced', max_iter = 10000, penalty = 'l1', solver = 'saga', C = 0.01, verbose=1)

"""## No Augmentation"""

train_x_scaled, test_x_scaled = scale_datasets(StandardScaler, df_train_x, [df_train_x, df_test_x])

if AUGMENT == 'NO':
    classifier.fit(train_x_scaled, train_y)
    y_hat_train = classifier.predict(train_x_scaled)
    train_res = precision_recall_fscore_support(train_y, y_hat_train)

else:
    augmented_train_x, augmented_train_y = resample_augment(df_train_x, train_y)

    #augmented_train_x_rescaled = scale_datasets(StandardScaler, augmented_train_x, [augmented_train_x])[0]

    augmented_train_x_rescaled, test_x_scaled = scale_datasets(StandardScaler, augmented_train_x, [augmented_train_x, df_test_x])
    augmented_train_x_rescaled, augmented_train_y = shuffle(augmented_train_x_rescaled, augmented_train_y)
    classifier.fit(augmented_train_x_rescaled, augmented_train_y)
    y_hat_train = classifier.predict(augmented_train_x_rescaled)
    train_res = precision_recall_fscore_support(augmented_train_y, y_hat_train, average='binary')

display_results(train_res)

y_hat_test = classifier.predict(test_x_scaled)
test_res = precision_recall_fscore_support(test_y, y_hat_test, average='binary')
display_results(test_res)

test_res = confusion_matrix(test_y, y_hat_test, labels=[1, 0])
print(test_res)
