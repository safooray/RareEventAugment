# -*- coding: utf-8 -*-
"""RareEvent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMX5_JMq1ptOJIFBJGtNYvXSuQ53RQSK
"""
from augmentation import *
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
from sklearn.utils import shuffle
import pandas as pd
import numpy as np

AUGMENT = 'YES'
"""## Helper Methods"""

def scale_datasets(scaler, train_set, to_scale):
  scaler = scaler().fit(train_set)
  res = []
  for dataset in to_scale:
    res.append(scaler.transform(dataset))
  return res

def display_results(precision_recall_fscore_support):
  disp_df = pd.DataFrame(data=precision_recall_fscore_support, columns=['Neg Class', 'Pos Class'], index=['Precision', 'Recall', 'F-score', 'support'])
  print(disp_df)


"""### Load Data"""

data_path = 'data/processminer-rare-event-detection-data-augmentation.xlsx'
data_file = pd.ExcelFile(data_path)
data = pd.read_excel(data_file, 'data-(b)-4-min-ahead-conse-rmvd')

"""### Train-Test-Validation Split"""

SEED = 0
DATA_SPLIT_PCT = 0.10
label_name = 'y-4min-ahead'

data = data.drop(['time', 'x28', 'x61'], axis=1)
df_train, df_test = train_test_split(data, test_size=DATA_SPLIT_PCT, random_state=SEED)
df_train, df_valid = train_test_split(df_train, test_size=DATA_SPLIT_PCT, random_state=SEED)
df_train_x = df_train.drop([label_name], axis=1)
df_valid_x = df_valid.drop([label_name], axis=1)
df_test_x = df_test.drop([label_name], axis=1)

train_y = df_train[label_name]
valid_y = df_valid[label_name]
test_y = df_test[label_name]

#TODO make vars for labels
classifier = LogisticRegression(class_weight='balanced', max_iter = 10000, penalty = 'l1', solver = 'saga', C = 0.01, verbose=1)

"""## No Augmentation"""

train_x_scaled, valid_x_scaled, test_x_scaled = scale_datasets(StandardScaler, df_train_x, [df_train_x, df_valid_x, df_test_x])

if AUGMENT == 'NO':
    classifier.fit(train_x_scaled, train_y)
    y_hat_train = classifier.predict(train_x_scaled)
    train_res = precision_recall_fscore_support(train_y, y_hat_train)

else:
    #augmented_train_x, augmented_train_y = resample_augment(df_train_x, train_y)

    #augmented_train_x_rescaled = scale_datasets(StandardScaler, augmented_train_x, [augmented_train_x])[0]

    augmented_train_x, augmented_train_y = noise_pos_augment(train_x_scaled, train_y)
    augmented_train_x, augmented_train_y = noise_pos_augment(augmented_train_x, augmented_train_y)
    augmented_train_x, augmented_train_y = noise_pos_augment(augmented_train_x, augmented_train_y)
    augmented_train_x, augmented_train_y = noise_pos_augment(augmented_train_x, augmented_train_y)

    augmented_train_x_rescaled, valid_x_scaled = scale_datasets(StandardScaler, augmented_train_x, [augmented_train_x, df_valid_x])
    augmented_train_x_rescaled, augmented_train_y = shuffle(augmented_train_x_rescaled, augmented_train_y)
    classifier.fit(augmented_train_x_rescaled, augmented_train_y)
    y_hat_train = classifier.predict(augmented_train_x_rescaled)
    train_res = precision_recall_fscore_support(augmented_train_y, y_hat_train)

display_results(train_res)
y_hat_valid = classifier.predict(valid_x_scaled)
val_res = precision_recall_fscore_support(valid_y, y_hat_valid)
display_results(val_res)

